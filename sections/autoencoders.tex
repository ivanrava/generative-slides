\begin{frame}
    \frametitle{Data compression}
    Key to deep learning is the concept of \emph{information compression}, as it allows the network to learn from more succinct representations and thus \emph{alleviating the overfitting} due to the \emph{curse of dimensionality}.

    \textbf{Autoencoders} are essentially used to compress the input data into a lower-dimensionality space.
\end{frame}

\begin{frame}
    \frametitle{Autoencoders}
    An \textbf{autoencoder} is made up of two parts: an encoder and a decoder, with the latent representation (often called \textbf{code}) in the middle.
    $$\text{Encoder} \to \text{Code} \to \text{Decoder}$$
    \img{latent-space}
\end{frame}

\begin{frame}
    \frametitle{Autoencoder loss}
    An autoencoder is usually trained through the minimization of a \textbf{loss} function like the following:
    $$L=\lVert x - d(e(x)) \rVert_2$$
    This effectively instructs the network to learn an effective compression strategy which is able to reconstruct the original higher-dimension data, at the highest possible fidelity, given the compressed information.
\end{frame}

\begin{frame}
    \frametitle{Latent space}
    \textbf{Latent representation} = spatially-reduced and high-level abstract representation of the target data. Usually found in the middle of an autoencoder architecture.

    Most generative models are unsupervised Deep Learning ones which learn to generate data from the learnt, \textbf{latent representations}.
    \img{latent-space}
\end{frame}

\begin{frame}
    \frametitle{Generation purpose}
    Once we have learned a good autoencoder we can split it into its two components only to keep using the \textbf{decoder} part of the network. Intuitively, given a randomly sampled latent vector in the latent space the decoder alone \emph{should} be able to produce a convincing image output resembling the original data distribution.
    \img{decoding}
\end{frame}

\begin{frame}
    \frametitle{Autoencoder latent space - issues}
    However, \textbf{the latent space learnt by an autoencoder is not regularized}. As such, sampling random points in this space does not warrant a meaningful output from the learned decoder.
    \imgv{ae_latent}
\end{frame}

\begin{frame}
    \frametitle{Autoencoders and PCA}
    Another interesting remark: the latent space learned by an autoencoder strongly resembles the eigenspace achieved through a PCA transform.

    Nevertheless, PCA is essentially a \emph{linear} transform, while autoencoders are able to learn much complex \emph{nonlinear} transformation maps.
    \imgv{ae_latent_pca}
\end{frame}

{
\setbeamercolor{background canvas}{bg=gray!20}
\begin{frame}
    \frametitle{Colab Notebook}
    Practical activities:
    \begin{itemize}
        \item Building and training an autoencoder over the Fashion MNIST dataset.
        \item Evaluating the model performance.
        \item Visualize the latent space in 2D.
    \end{itemize}
    \colab{colab}
\end{frame}
}

\subsection{Variational Autoencoders}

\begin{frame}
    \frametitle{Variational Autoencoders}
\end{frame}